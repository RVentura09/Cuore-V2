{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-zEK0LQCE_Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # Library to help us with plots\n",
    "from sklearn.preprocessing import MinMaxScaler # Library to help us scale\n",
    "from sklearn.model_selection import train_test_split # Library to help us split the dataset\n",
    "from sklearn.ensemble import RandomForestClassifier # Library to enable Random Forest\n",
    "from sklearn.svm import SVC # Library to enable support vector machine\n",
    "from sklearn.neighbors import KNeighborsClassifier # Library to enable KNN\n",
    "from sklearn.tree import DecisionTreeClassifier # Library to enable decision tree\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score ,roc_curve\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "import pickle # Library to help us save the model\n",
    "\n",
    "RANDOM_SEED = 1331\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4oNisdzfjcg"
   },
   "source": [
    "Based from the original Cuore project  by:\n",
    "Frank Aiwuyor Ogiemwonyi, Rony Ventura, Tara de Groot,\n",
    "Eric Vincent Rivas, Silvia Dubon, Dwi Aji Kurnia Putra, and Laureanne van Dijk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV9LUODKSVBE"
   },
   "source": [
    "## 1. Import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "PqzMb5R7SMyY",
    "outputId": "28987291-becd-412e-c6c6-fb9257c4bdcb"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed.cleveland.csv\",header=None) # Opening and reading the file of Cleveland\n",
    "df.head() # View of first 5 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding column names\n",
    "#Column names referenced from original dataset\n",
    "#https://archive.ics.uci.edu/dataset/45/heart+disease\n",
    "df=df.set_axis(['Age', 'Sex', 'ChestPain', 'Trestbps', 'Chol', 'Fbs', 'Restecg', 'Thalach', 'Exang', 'Oldpeak', 'Slope', 'Ca', 'Thal', 'Class'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values and data types of each column\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\\n\" + \" Data set shape is :\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of the data to check for outliers especially on the age column \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wo0RIVNSSiK"
   },
   "source": [
    "## 2. Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ymR8zT6Sogq"
   },
   "source": [
    "### 2.1 Delete rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hl3x4o79Sbcl",
    "outputId": "3fa3a7a2-8da6-4b10-c108-233c0e355248"
   },
   "outputs": [],
   "source": [
    "# As Features \"CA\" and \"THAL\" are type object lets check what unique values they have \n",
    "print(df['Ca'].unique()) # Print unique values of \"ca\"\n",
    "print(df['Thal'].unique()) # Print unique values of \"thal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkd3xv6SSgSU",
    "outputId": "af08eb1e-92e8-4840-d88d-4fd8228d745e"
   },
   "outputs": [],
   "source": [
    "# Missing values are stated with a questionmark, so let's see how many missing values we have\n",
    "print(\" \\\"?\\\" values in the dataframe \") # Print the phrase \"?\" values in the dataframe\n",
    "print((df == \"?\").sum(axis=0)) # Finding the number of \"?\" in each feature (column) and print the sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tb_ySjh7SipZ",
    "outputId": "46d3ac90-0a1b-4717-e7d2-4a3ecfbb5ff1"
   },
   "outputs": [],
   "source": [
    "# Creating a new df in which the rows with values \"?\" from the columns \"ca\" and \"thal\" are excluded\n",
    "df = df[(df[\"Ca\"] != '?') & (df[\"Thal\"] != '?')]#This line will use only the rows that does not contain ?\n",
    "#In case we had empty values in the form of nan or null: \n",
    "#df.dropna(inplace = True) #  Deletion of null values \n",
    "print(\" \\\"?\\\" values in the dataframe \") # Print the phrase \"?\" values in the dataframe\n",
    "# Finding the number of \"?\" in each feature (column) and showing (printing) the sum of it to verify there are no more \"?\" values\n",
    "print((df == \"?\").sum(axis=0)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boy66zrGIGYn"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True) # Reset the index to make sure the index matches the number of entries \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of the dataframe to work in the machine learning model \n",
    "#The transformation of Data is done in python instead of PowerBi to ease up the reproducibility and demonstrate the procedure\n",
    "dfmodel=df.copy()\n",
    "dfmodel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that the data is clean we have to convert to the correct datatype\n",
    "df[\"Age\"] = df[\"Age\"].astype(\"int\")\n",
    "df[\"Sex\"] = df[\"Sex\"].astype(\"category\")\n",
    "df[\"ChestPain\"] = df[\"ChestPain\"].astype(\"category\")\n",
    "df[\"Fbs\"] = df[\"Fbs\"].astype(\"category\")\n",
    "df[\"Restecg\"] = df[\"Restecg\"].astype(\"category\")\n",
    "df[\"Thalach\"] = df[\"Thalach\"].astype(\"int\")\n",
    "df[\"Exang\"] = df[\"Exang\"].astype(\"category\")\n",
    "df[\"Slope\"] = df[\"Slope\"].astype(\"category\")\n",
    "df[\"Ca\"] = pd.to_numeric(df['Ca'])\n",
    "df[\"Ca\"]=df[\"Ca\"].astype(\"int\")\n",
    "df[\"Thal\"] = pd.to_numeric(df['Thal'])\n",
    "df[\"Thal\"] = df[\"Thal\"].astype(\"category\")\n",
    "df[\"Class\"]=df[\"Class\"].astype(\"category\")\n",
    "# Rename values \n",
    "df['Sex'] = df['Sex'].replace({0: 'Female', 1: 'Male'})\n",
    "df['ChestPain'] = df['ChestPain'].replace({1: 'Typical angina', 2: 'Atypical angina',3:'Non-anginal pain',4:'Asymptomatic'})\n",
    "df['Fbs'] = df['Fbs'].replace({0: 'Normal', 1: 'High'})\n",
    "df['Restecg'] = df['Restecg'].replace({0: 'Normal', 1: 'Abnormal',2:'Probable hypertrophy'})\n",
    "df['Exang'] = df['Exang'].replace({0: 'No', 1: 'Yes'})\n",
    "df['Slope'] = df['Slope'].replace({1: 'Upsloping', 2: 'Flat',3:'Downsloping'})\n",
    "df['Thal'] = df['Thal'].replace({3.0: 'Normal',6:'Fixed defect',7:'Reversable defect'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcEkSiwhxg1Y",
    "outputId": "6ee8c1fc-2603-4631-d170-e10b8874aff0"
   },
   "outputs": [],
   "source": [
    "df.shape # Size after deletion of missing values\n",
    "#After the cleaning phase we have deleted 6 rows in total ,approximately 1.98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ux4WaTkObdin"
   },
   "source": [
    "### 2.3 Check Class labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class= 0: Healthy \\\n",
    "Class=> 1: Not *healthy* \\\n",
    "Prediction outcome will categorize  healthy or not healthy , therefore class 1,2,3,4 will be considered not healthy and grouped together in one single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class\"].value_counts() # Count and show the number of values per class label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing values from 1-4 to 1, because 1 means they have a heart disease i.e. not healthy \n",
    "df[\"Class\"].replace({2: 1, 3: 1, 4:1}, inplace=True) \n",
    "dfmodel[\"Class\"].replace({2: 1, 3: 1, 4:1}, inplace=True)#replace the class labels in the models dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class\"].value_counts() # Count the amount of values for each class label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the values of Class column\n",
    "df['Class'] = df['Class'].replace({0: 'Healthy', 1: 'Not Healthy'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Save the CSVs\n",
    "For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Cleveland_TransformedData.csv', index=False)\n",
    "dfmodel.to_csv('Cleveland_ModelData.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEA3LEemSz_U"
   },
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small data analysis is performed in python just to grasp a notion of what is behind the data.\n",
    "A dedicated data analysis is done in Power Bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1Xq-Bt6cDwN"
   },
   "source": [
    "Comparison of women vs men "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "bKkQayDtcx3c",
    "outputId": "4eec8f26-63f5-4c4b-a620-34331e0c829a"
   },
   "outputs": [],
   "source": [
    "# We are going to plot the negative and postive values (healty/not healthy) grouped by sex\n",
    "# This way we can compare the outcome between women and men\n",
    "ylabels = ['Healthy', 'Not Healthy'] # The labels we use for the y-axis\n",
    "#labels = (\"female\", \"male\") # The labels we use for the x-axis\n",
    "\n",
    "positions = (0, 1)\n",
    "\n",
    "Sex = dfmodel.groupby(\"Sex\")['Class'].value_counts() # We're grouping by sex\n",
    "ax = Sex.unstack().plot(kind='bar',legend=False, rot=0) # Creating the barplot\n",
    "plt.legend(labels=ylabels) # Create the legend based on who is healthy/not healthy\n",
    "plt.show() # show the plot\n",
    "Sex # Print the values we see in the plot so we're able to check the exact values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUS0sVXa4AEO"
   },
   "source": [
    "## 4. Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDIGRppfMT6_"
   },
   "outputs": [],
   "source": [
    "# Define the columns in which the values are numerical \n",
    "numerical = [\"Age\", \"Trestbps\", \"Chol\", \"Thalach\", \"Oldpeak\", \"Ca\"]\n",
    "X = dfmodel[numerical] # Create a dataframe for standardization that only includes the numerical features\n",
    "# We compared the evaluation metrics of MinMaxScaler() and StandardScaler()\n",
    "# MinMaxScaler() scored slightly better so we decided to go with MinMaxScaler()\n",
    "scaler=MinMaxScaler() # Standardize the dataset\n",
    "df_standard = scaler.fit_transform(X) #fit and transform the data\n",
    "# Create a dataframe with the standardized values\n",
    "df_standard = pd.DataFrame(df_standard, columns=numerical) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8ZlWeJahzSwT",
    "outputId": "c201cf23-6816-4f73-d99c-a0bc58ed2de1"
   },
   "outputs": [],
   "source": [
    "# Combine original dataframe with standardized dataset\n",
    "standardized = dfmodel.copy() # Create a copy of the origional dataset\n",
    "# Replace the numerical features in the original dataframe with the standardized features\n",
    "standardized[numerical] = df_standard[numerical] \n",
    "standardized.head() # Print the first 5 rows of the standardized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdx8Q4974E5C"
   },
   "source": [
    "## 5. Selecting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUeSO3mm0aM3"
   },
   "outputs": [],
   "source": [
    "# Create 2 test sets: one with all features and one with selected features\n",
    "# First we create a dataframe in which all features are included\n",
    "X_all_features = standardized.copy() # Create a copy of the standardized dataframe\n",
    "X_all_features.drop(columns=['Class'],inplace=True) # Drop the outcome variable\n",
    "# Create a dataset with only the selected values\n",
    "X_selection = standardized[['Chol', 'Age', 'Fbs', 'Trestbps', 'Ca']]\n",
    "X_selection = pd.DataFrame(X_selection) # Converting the X_selection to a dataframe\n",
    "y = standardized['Class'] # We store the classlabels in y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niDYUd4k4J0z"
   },
   "source": [
    "## 6. Splitting in training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJ9NJkYS1_sP"
   },
   "outputs": [],
   "source": [
    "# Split the dataset that includes all features into a training and test set\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_all_features, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# Split the dataset that includes a selection of features into a training and test set\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X_selection, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFxQNtJ3OxmA"
   },
   "outputs": [],
   "source": [
    "# Defining a method to print the size of the test/training set\n",
    "def size(X_train, X_test, y_train, y_test):\n",
    "  print(\"The size of the training set is: \",X_train.shape)\n",
    "  print(\"The size of the test set is: \",X_test.shape)\n",
    "  print(\"The size of the training target set is: \",y_train.shape)\n",
    "  print(\"The size of the test target set is: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bt8gJkEB0Hwo",
    "outputId": "26e0d50d-1940-44c3-f107-f606867361ae"
   },
   "outputs": [],
   "source": [
    "# Checking if the train/split sets match with target of the dataset with all features\n",
    "size(X1_train, X1_test, y1_train, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWZFmtgh0LZ4",
    "outputId": "f78bd6b7-4053-40aa-9223-78ff61d6473d"
   },
   "outputs": [],
   "source": [
    "# Checking if the train / split  sets match with target of the dataset with the feature selection\n",
    "size(X2_train, X2_test, y2_train, y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtwAl-4UUn2p"
   },
   "source": [
    "## 7. Choosing models Random Forest, SVM, KNN, Descision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FS2qBH0hwPQq"
   },
   "outputs": [],
   "source": [
    "# Define classifiers to train and test \n",
    "MODELS_TO_TEST = {\n",
    "    \"RF_10\": RandomForestClassifier(n_estimators=10, max_depth=5),\n",
    "    \"SVM\": SVC(kernel='linear'),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"DT\": DecisionTreeClassifier(max_depth=3),\n",
    "}\n",
    "\n",
    "# Define the number of splits \n",
    "NUMBER_OF_SPLITS = 5\n",
    "\n",
    "# Define the scoring metrics\n",
    "SCORING_METRICS = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"roc_auc\"] # Metrics of interest\n",
    "\n",
    "# Create empty DataFrame to populate the name of the classifier and the six values returned from `cross_validate()`\n",
    "results_evaluation = pd.DataFrame({\n",
    "                                    \"classifier_name\":[],\n",
    "                                    \"fit_time\": [],\n",
    "                                    \"score_time\": [],\n",
    "                                    \"test_accuracy\": [],\n",
    "                                    \"test_precision_macro\": [],\n",
    "                                    \"test_recall_macro\": [],\n",
    "                                    \"test_f1_macro\": [],\n",
    "                                    \"test_roc_auc\": [],\n",
    "                                    })\n",
    "results_evaluation_selected_features = pd.DataFrame({\n",
    "                                    \"classifier_name\":[],\n",
    "                                    \"fit_time\": [],\n",
    "                                    \"score_time\": [],\n",
    "                                    \"test_accuracy\": [],\n",
    "                                    \"test_precision_macro\": [],\n",
    "                                    \"test_recall_macro\": [],\n",
    "                                    \"test_f1_macro\": [],\n",
    "                                    \"test_roc_auc\": [],\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QosbVTzWQsrL"
   },
   "source": [
    "## 8. Train, test and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGfarTY15n09",
    "outputId": "e46ad281-d747-4f52-e5aa-d467ea8757ce"
   },
   "outputs": [],
   "source": [
    "#### ITERATION FOR THE EXPERIMENT for Dataset with ALL features\n",
    "\n",
    "for name, classifier in MODELS_TO_TEST.items():\n",
    "    \n",
    "    print(f\"Currently training the classifier {name}.\")\n",
    "\n",
    "    # Get the evaluation metrics per fold after cross-validation\n",
    "    # Note that we are passing the normalized array `data_X_norm` to all classifiers\n",
    "    scores_cv = cross_validate(classifier, X_all_features, y, cv=NUMBER_OF_SPLITS, scoring=SCORING_METRICS)\n",
    "\n",
    "    # Average the scores among folds\n",
    "    dict_this_result = {\n",
    "                    \"classifier_name\":[name],\n",
    "                    }\n",
    "    # Populate the dictionary with the results of the cross-validation\n",
    "    for metric_name, score_per_fold in scores_cv.items():\n",
    "        dict_this_result[metric_name] = [ scores_cv[metric_name].mean() ]\n",
    "\n",
    "    # Generate the results to populate the pandas.DataFrame\n",
    "    this_result = pd.DataFrame(dict_this_result)\n",
    "\n",
    "    # Append to the main dataframe with the results \n",
    "    results_evaluation = pd.concat([results_evaluation, this_result], ignore_index=True)\n",
    "\n",
    "print(\"The experimental setup has finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "1_zc4_SCE_XY",
    "outputId": "3d6c00f2-d524-43c1-ca76-0d1bd7b69098"
   },
   "outputs": [],
   "source": [
    "# Printing the evaluation metrics of all features\n",
    "results_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqivXnGTFV6n"
   },
   "outputs": [],
   "source": [
    "# Store the average accuracy from the all features dataset and group by the name of the classifier\n",
    "average_score_classifier = results_evaluation.groupby(by=[\"classifier_name\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "vCGPN8mXFZOM",
    "outputId": "a3ff0655-27f9-4808-965d-52aca78cf923"
   },
   "outputs": [],
   "source": [
    "average_score_classifier[\"test_accuracy\"].plot.bar() # Plot the the average accuracy from the all features dataset\n",
    "plt.title(\"Average accuracy per classifier among dataset\") # Set a title for the plot\n",
    "plt.xlabel(\"Classifiers\") # Set a label for the X-axis\n",
    "plt.show() # Show the plot\n",
    "average_score_classifier[\"test_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "u0c_bvhH_VvI",
    "outputId": "f246b81e-ceb9-49da-b192-8660c866b2ab"
   },
   "outputs": [],
   "source": [
    "average_score_classifier[\"test_roc_auc\"].plot.bar() # Plot the the average accuracy from the all features dataset\n",
    "plt.title(\"Average roc-auc per classifier among dataset\") # Set a title for the plot\n",
    "plt.xlabel(\"Classifiers\") # Set a label for the X-axis\n",
    "plt.show() # Show the plot\n",
    "average_score_classifier[\"test_roc_auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeTAZscnGVpf",
    "outputId": "00f6c345-13c5-4a86-ab14-75dfd862a2dc"
   },
   "outputs": [],
   "source": [
    "#### ITERATION FOR THE EXPERIMENT for Dataset with selected features\n",
    "\n",
    "for name, classifier in MODELS_TO_TEST.items():\n",
    "    \n",
    "    print(f\"Currently training the classifier {name}.\")\n",
    "\n",
    "    # Get the evaluation metrics per fold after cross-validation\n",
    "    # Note that we are passing the normalized array `data_X_norm` to all classifiers\n",
    "    scores_cv_selected = cross_validate(classifier, X_selection, y, cv=NUMBER_OF_SPLITS, scoring=SCORING_METRICS)\n",
    "\n",
    "    # Average the scores among folds\n",
    "    dict_this_result_selected = {\n",
    "                    \"classifier_name\":[name],\n",
    "                    }\n",
    "    # Populate the dictionary with the results of the cross-validation\n",
    "    for metric_name, score_per_fold in scores_cv_selected.items():\n",
    "        dict_this_result_selected[metric_name] = [ scores_cv_selected[metric_name].mean() ]\n",
    "\n",
    "    #### Generate the results to populate the pandas.DataFrame\n",
    "    this_result_selected = pd.DataFrame(dict_this_result_selected)\n",
    "\n",
    "    # Append to the main dataframe with the results \n",
    "    results_evaluation_selected_features = pd.concat([results_evaluation_selected_features, this_result_selected], ignore_index=True)\n",
    "\n",
    "print(\"The experimental setup has finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "Wd4-rD3jG4Qs",
    "outputId": "00660682-817a-40bf-bc87-899cf681e0ec"
   },
   "outputs": [],
   "source": [
    "# Printing the evaluation metrics of the selected features\n",
    "results_evaluation_selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPpgWTy6EISa"
   },
   "source": [
    "## 9. Tune the models for better performance by optimizing the parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvj_ufcN6Rzp"
   },
   "outputs": [],
   "source": [
    "# Create a standard SVC classifier clf without any parameter\n",
    "clf = SVC()\n",
    "# Grid search with a list of two parameter dictionaries, one with kernel = ['poly'] and degree = [2, 3, 4]\n",
    "# and the other one with kernel = ['linear', 'rbf'] and C = [1, 10, 100, 1000]\n",
    "param_grid = [\n",
    "    {'kernel': ['poly'], 'degree': [2, 3, 4]},\n",
    "    {'kernel': ['linear', 'rbf'], 'C': [1, 10, 100, 1000]},\n",
    "    ]\n",
    "# Run GridSearch and save the scores\n",
    "gs = GridSearchCV(clf, param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7r5HAfWDoh7K",
    "outputId": "2b31b04e-3992-44d8-ba2c-1e2479fb34c0"
   },
   "outputs": [],
   "source": [
    "# Fit the training data of the dataset with all features\n",
    "gs.fit(X1_train, y1_train)\n",
    "# The best classifier\n",
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dar9ElBnbZLp",
    "outputId": "61304ec1-5bc5-41e8-c1ab-82bc23944c06"
   },
   "outputs": [],
   "source": [
    "# The best score\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbd-zpT-Dh32"
   },
   "source": [
    "## 10. Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWDQxfhdHlCj",
    "outputId": "12355bc8-ea01-4f6e-f8cd-2c92aefa44a5"
   },
   "outputs": [],
   "source": [
    "# Predict model with gs (best model)\n",
    "y1_predicted = gs.predict(X1_test)\n",
    "print(classification_report(y1_test, y1_predicted))\n",
    "print(f'model  AUC score: {roc_auc_score(y1_test, y1_predicted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnDAtoEtENpg"
   },
   "outputs": [],
   "source": [
    "# Method for plotting AUC/ROC curve\n",
    "def plot_roc_curve(true_y, y_prob): \n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y1_test, y1_predicted)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "MXfY9bgxEVbw",
    "outputId": "14b52096-cea5-4231-ef94-873bc36ea0e9"
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(y1_test, y1_predicted) # Plot AUC/ROC curve\n",
    "print(f'model  AUC score: {roc_auc_score(y1_test, y1_predicted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCFxPBFJDnEl"
   },
   "outputs": [],
   "source": [
    "# Save the trained model into trained_model_cuore.pickle using pickle, without using a folder_path\n",
    "FOLDER_PATH = \"\"\n",
    "trained_model_filename = FOLDER_PATH + \"trained_model_cuore.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoFVKZ8AEK8Q"
   },
   "outputs": [],
   "source": [
    "# Create file with the specific variable in the specified folder\n",
    "data_to_save = gs.best_estimator_ \n",
    "file_path = trained_model_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xl603n58ERhq"
   },
   "outputs": [],
   "source": [
    "# Creates a binary object and writes the indicated variables\n",
    "with open(file_path, \"wb\") as writeFile:\n",
    "    pickle.dump(data_to_save, writeFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zIinXEMLg9f"
   },
   "outputs": [],
   "source": [
    "# Here we will load the same model, but in a variable that is completely empty\n",
    "loaded_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXip5FMxQEPl"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(trained_model_filename, \"rb\") as readFile:\n",
    "    loaded_model = pickle.load(readFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MsiqgTaTQQZ1",
    "outputId": "84730ffd-e353-4652-f947-f61ba379a7a9"
   },
   "outputs": [],
   "source": [
    "Y_predicted_loaded_model = loaded_model.predict(X1_test) # Confirm that the loaded model has the same metrics as the one trained\n",
    "print(classification_report(y1_test, Y_predicted_loaded_model))\n",
    "print(f'model  AUC score: {roc_auc_score(y1_test, Y_predicted_loaded_model)}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbbba652b4feaf285c555c8e0526df41e56971e88a0edaccf2a040884b4841ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
